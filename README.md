
# End-to-End Data Pipeline with Docker, PostgreSQL, and ELT

This project demonstrates an **ELT (Extract, Load, Transform)** pipeline to migrate data between two PostgreSQL databases using Docker. 
The pipeline automates the extraction of data from a source database, loading it into a destination database, and includes error handling and retry logic for robustness. 
It also integrates **dbt (Data Build Tool)** for data transformation and provides a foundation for orchestration using **Airflow** and **cron jobs**.
Why both cron and Airflow, well cron is for basic scheduling once every 3 hours with no dynamic workflows whereas Airflow can make dynamic workflows with code integration and allowing for complex dependencies and conditional logic. 

---

## 🚀 Features
- **Docker Compose Setup**: Orchestrates PostgreSQL databases and ELT script containers.
- **ELT Automation**: Uses `pg_dump` and `psql` to transfer data between databases.
- **Sample Data Initialization**: Populates the source database with tables for `users`, `films`, `actors`, and relationships.
- **Error Handling**: Retries failed connections and logs errors for debugging.
- **dbt Integration**: Used macros in DBT to create a CTE(Common Table Expression) for creating film_ratings table based on the others.
- **Orchestration**: Supports scheduling with **Airflow** and **cron jobs** as well.

---

## 📂 Project Structure
```
.
├── airflow/                            # Airflow DAGs and configuration
│   ├── dags/                           # Directory for Airflow DAGs
│   │   └── elt_dag.py                  # Airflow DAG for ELT pipeline
│   └── airflow.cfg                     # Airflow configuration file
├── dbt/                                # dbt project for data transformation
│   ├── analyses/                       # Directory for dbt analyses
│   ├── dbt_packages/                   # Installed dbt packages
│   ├── logs/                           # Logs generated by dbt
│   │   └── dbt.log                     # Log file for dbt runs
│   ├── macros/                         # Custom dbt macros
│   │   └── .gitkeep                    # Placeholder file
│       └── films_ratings_macros.sql    # CTE for creating ratings based on different tables.
|   ├── models/                         # dbt models for data transformation
│   │   └── example/                    # Example models
│   │       ├── actors.sql              # Model for actors table
│   │       ├── film_actors.sql         # Model for film_actors table
│   │       ├── film_ratings.sql        # Model for film_ratings table
│   │       ├── films.sql               # Model for films table
│   │       ├── schema.yml              # Schema definitions for models
│   │       └── sources.yml             # Source definitions for models
│   ├── seeds/                          # Seed data for dbt
│   ├── snapshots/                      # dbt snapshots for slowly changing dimensions
│   ├── target/                         # Compiled dbt artifacts
│   ├── tests/                          # dbt tests
│   ├── .gitignore                      # Git ignore file for dbt
│   └── dbt_project.yml                 # dbt project configuration
├── ELT/                                # ELT script and Docker setup
│   ├── Dockerfile                      # Dockerfile for ELT script
│   ├── elt_script.py                   # Python script for ELT process
│   ├── logs/                           # Logs for ELT script
│   │   └── dbt.log                     # Log file for ELT script
│   └── source_db_init/                 # Initialization scripts for source database
│       └── init.sql                    # SQL script to create tables and insert sample data
├── docker-compose.yaml                 # Docker Compose configuration for the project
├── Dockerfile                          # Main Dockerfile for the project
├── README.md                           # Project documentation
└── start.sh                            # Script from CRON jobs
```


## ⚙️ How It Works

### 1. **Docker Compose Setup**
Three services are defined in `docker-compose.yaml`:
- `source_postgres`: Source PostgreSQL database initialized with `init.sql`.
- `destination_postgres`: Destination PostgreSQL database (empty initially).
- `elt_script`: Runs the ELT script to migrate data from source to destination.
- `airflow`: Airflow service for scheduling the ELT pipeline.

### 2. **ELT Process**
The `elt_script.py` performs the following steps:
1. **Wait for Source Database**: Uses `pg_isready` to ensure the source database is available.
2. **Extract Data**: Executes `pg_dump` to create a SQL dump of the source database.
3. **Load Data**: Uses `psql` to load the dumped SQL file into the destination database.

### 3. **Database Initialization**
The `init.sql` script:
- Creates tables: `users`, `films`, `actors`, `film_actors`, `film_category`.
- Inserts sample data for testing (e.g., 20 actors, 14 users, 20 films).

### 4. **dbt Integration**
- **Transform Data**: After loading data into the destination database, dbt models are used to transform and analyze the data.
- **Example dbt Model**:
  ```sql
  -- models/example/actors.sql
  SELECT * FROM {{ source('destination_db', 'actors') }}
  ```

### 5. **Orchestration**
- **Airflow**: Used Airflow to schedule and monitor the ELT pipeline. Example DAGs can be added to the `airflow/` directory.
- **Cron Jobs**: Schedule the ELT script to run periodically using cron jobs.

---

## 🛠️ Installation

### Prerequisites
- Docker and Docker Compose installed.
- Basic command-line knowledge.

### Steps
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/end-to-end-data-pipeline.git
   cd end-to-end-data-pipeline
   ```

2. **Start the Containers**:
   ```bash
   docker-compose up --build
   ```

3. **Verify Data Transfer**:
   - Connect to the destination database:
     ```bash
     docker exec -it destination_postgres psql -U postgres -d destination_db
     ```
   - Run queries to check tables:
     ```sql
     \dt
     SELECT * FROM actors;
     ```

4. **Run dbt**:
   - Navigate to the `dbt/` directory and run:
     ```bash
     dbt run
     ```

5. **Orchestration**:
   - For **Airflow**, set up the Airflow container and add DAGs to the `airflow/` directory.
   - For **cron jobs**, add a cron job to run the ELT script periodically:
     ```bash
     crontab -e
     # Add the following line to run the script every day at midnight
     0 0 * * * docker compose run elt_script
     ```

---

## 🔍 Example Data Models
The source database (`source_db`) is initialized with the following tables:
- **users**: User details (name, email, date of birth).
- **films**: Movie information (title, release date, price, rating).
- **actors**: Actor names and IDs.
- **film_actors**: Mapping of actors to films.
- **film_category**: Categories for films (e.g., Action, Comedy).

---

## 🚨 Troubleshooting
- **PostgreSQL Connection Issues**: Ensure both databases are running (`docker ps`).
- **Permission Errors**: Verify Docker volume mounts and file permissions.
- **dbt Errors**: Ensure `profiles.yml` is correctly configured and the destination database is accessible.

---

## 📈 Future Enhancements
1. **Data Validation**: Add tests to verify data consistency between source and destination.
2. **Monitoring**: Integrate monitoring tools like Prometheus and Grafana.
3. **Scalability**: Extend the pipeline to handle larger datasets and multiple data sources.

---

## 📜 License
MIT License. See [LICENSE](LICENSE) for details.

---

## 🙌 Acknowledgments
- PostgreSQL and Docker communities for tooling support.
- `psycopg2` and `pg_dump` for enabling seamless data migration.
- dbt and Airflow communities for transformation and orchestration tools.
```

This README provides a comprehensive overview of the project, setup instructions, and future goals. Adapt the GitHub URL, license, or acknowledgments as needed! 🚀