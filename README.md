
# End-to-End Data Pipeline with Docker, PostgreSQL, and ELT

This project demonstrates an **ELT (Extract, Load, Transform)** pipeline to migrate data between two PostgreSQL databases using Docker. 
The pipeline automates the extraction of data from a source database, loading it into a destination database, and includes error handling and retry logic for robustness. 
It also integrates **dbt (Data Build Tool)** for data transformation and provides a foundation for orchestration using **Airflow** and **cron jobs**.
Why both cron and Airflow, well cron is for basic scheduling once every 3 hours with no dynamic workflows whereas Airflow can make dynamic workflows with code integration and allowing for complex dependencies and conditional logic. 

---

## ğŸš€ Features
- **Docker Compose Setup**: Orchestrates PostgreSQL databases and ELT script containers.
- **ELT Automation**: Uses `pg_dump` and `psql` to transfer data between databases.
- **Sample Data Initialization**: Populates the source database with tables for `users`, `films`, `actors`, and relationships.
- **Error Handling**: Retries failed connections and logs errors for debugging.
- **dbt Integration**: Used macros in DBT to create a CTE(Common Table Expression) for creating film_ratings table based on the others.
- **Orchestration**: Supports scheduling with **Airflow** and **cron jobs** as well.

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ airflow/                            # Airflow DAGs and configuration
â”‚   â”œâ”€â”€ dags/                           # Directory for Airflow DAGs
â”‚   â”‚   â””â”€â”€ elt_dag.py                  # Airflow DAG for ELT pipeline
â”‚   â””â”€â”€ airflow.cfg                     # Airflow configuration file
â”œâ”€â”€ dbt/                                # dbt project for data transformation
â”‚   â”œâ”€â”€ analyses/                       # Directory for dbt analyses
â”‚   â”œâ”€â”€ dbt_packages/                   # Installed dbt packages
â”‚   â”œâ”€â”€ logs/                           # Logs generated by dbt
â”‚   â”‚   â””â”€â”€ dbt.log                     # Log file for dbt runs
â”‚   â”œâ”€â”€ macros/                         # Custom dbt macros
â”‚   â”‚   â””â”€â”€ .gitkeep                    # Placeholder file
â”‚       â””â”€â”€ films_ratings_macros.sql    # CTE for creating ratings based on different tables.
|   â”œâ”€â”€ models/                         # dbt models for data transformation
â”‚   â”‚   â””â”€â”€ example/                    # Example models
â”‚   â”‚       â”œâ”€â”€ actors.sql              # Model for actors table
â”‚   â”‚       â”œâ”€â”€ film_actors.sql         # Model for film_actors table
â”‚   â”‚       â”œâ”€â”€ film_ratings.sql        # Model for film_ratings table
â”‚   â”‚       â”œâ”€â”€ films.sql               # Model for films table
â”‚   â”‚       â”œâ”€â”€ schema.yml              # Schema definitions for models
â”‚   â”‚       â””â”€â”€ sources.yml             # Source definitions for models
â”‚   â”œâ”€â”€ seeds/                          # Seed data for dbt
â”‚   â”œâ”€â”€ snapshots/                      # dbt snapshots for slowly changing dimensions
â”‚   â”œâ”€â”€ target/                         # Compiled dbt artifacts
â”‚   â”œâ”€â”€ tests/                          # dbt tests
â”‚   â”œâ”€â”€ .gitignore                      # Git ignore file for dbt
â”‚   â””â”€â”€ dbt_project.yml                 # dbt project configuration
â”œâ”€â”€ ELT/                                # ELT script and Docker setup
â”‚   â”œâ”€â”€ Dockerfile                      # Dockerfile for ELT script
â”‚   â”œâ”€â”€ elt_script.py                   # Python script for ELT process
â”‚   â”œâ”€â”€ logs/                           # Logs for ELT script
â”‚   â”‚   â””â”€â”€ dbt.log                     # Log file for ELT script
â”‚   â””â”€â”€ source_db_init/                 # Initialization scripts for source database
â”‚       â””â”€â”€ init.sql                    # SQL script to create tables and insert sample data
â”œâ”€â”€ docker-compose.yaml                 # Docker Compose configuration for the project
â”œâ”€â”€ Dockerfile                          # Main Dockerfile for the project
â”œâ”€â”€ README.md                           # Project documentation
â””â”€â”€ start.sh                            # Script from CRON jobs
```


## âš™ï¸ How It Works

### 1. **Docker Compose Setup**
Three services are defined in `docker-compose.yaml`:
- `source_postgres`: Source PostgreSQL database initialized with `init.sql`.
- `destination_postgres`: Destination PostgreSQL database (empty initially).
- `elt_script`: Runs the ELT script to migrate data from source to destination.
- `airflow`: Airflow service for scheduling the ELT pipeline.

### 2. **ELT Process**
The `elt_script.py` performs the following steps:
1. **Wait for Source Database**: Uses `pg_isready` to ensure the source database is available.
2. **Extract Data**: Executes `pg_dump` to create a SQL dump of the source database.
3. **Load Data**: Uses `psql` to load the dumped SQL file into the destination database.

### 3. **Database Initialization**
The `init.sql` script:
- Creates tables: `users`, `films`, `actors`, `film_actors`, `film_category`.
- Inserts sample data for testing (e.g., 20 actors, 14 users, 20 films).

### 4. **dbt Integration**
- **Transform Data**: After loading data into the destination database, dbt models are used to transform and analyze the data.
- **Example dbt Model**:
  ```sql
  -- models/example/actors.sql
  SELECT * FROM {{ source('destination_db', 'actors') }}
  ```

### 5. **Orchestration**
- **Airflow**: Used Airflow to schedule and monitor the ELT pipeline. Example DAGs can be added to the `airflow/` directory.
- **Cron Jobs**: Schedule the ELT script to run periodically using cron jobs.

---

## ğŸ› ï¸ Installation

### Prerequisites
- Docker and Docker Compose installed.
- Basic command-line knowledge.

### Steps
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/end-to-end-data-pipeline.git
   cd end-to-end-data-pipeline
   ```

2. **Start the Containers**:
   ```bash
   docker-compose up --build
   ```

3. **Verify Data Transfer**:
   - Connect to the destination database:
     ```bash
     docker exec -it destination_postgres psql -U postgres -d destination_db
     ```
   - Run queries to check tables:
     ```sql
     \dt
     SELECT * FROM actors;
     ```

4. **Run dbt**:
   - Navigate to the `dbt/` directory and run:
     ```bash
     dbt run
     ```

5. **Orchestration**:
   - For **Airflow**, set up the Airflow container and add DAGs to the `airflow/` directory.
   - For **cron jobs**, add a cron job to run the ELT script periodically:
     ```bash
     crontab -e
     # Add the following line to run the script every day at midnight
     0 0 * * * docker compose run elt_script
     ```

---

## ğŸ” Example Data Models
The source database (`source_db`) is initialized with the following tables:
- **users**: User details (name, email, date of birth).
- **films**: Movie information (title, release date, price, rating).
- **actors**: Actor names and IDs.
- **film_actors**: Mapping of actors to films.
- **film_category**: Categories for films (e.g., Action, Comedy).

---

## ğŸš¨ Troubleshooting
- **PostgreSQL Connection Issues**: Ensure both databases are running (`docker ps`).
- **Permission Errors**: Verify Docker volume mounts and file permissions.
- **dbt Errors**: Ensure `profiles.yml` is correctly configured and the destination database is accessible.

---

## ğŸ“ˆ Future Enhancements
1. **Data Validation**: Add tests to verify data consistency between source and destination.
2. **Monitoring**: Integrate monitoring tools like Prometheus and Grafana.
3. **Scalability**: Extend the pipeline to handle larger datasets and multiple data sources.

---

## ğŸ“œ License
MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™Œ Acknowledgments
- PostgreSQL and Docker communities for tooling support.
- `psycopg2` and `pg_dump` for enabling seamless data migration.
- dbt and Airflow communities for transformation and orchestration tools.
```

This README provides a comprehensive overview of the project, setup instructions, and future goals. Adapt the GitHub URL, license, or acknowledgments as needed! ğŸš€